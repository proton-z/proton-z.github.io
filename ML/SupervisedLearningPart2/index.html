

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#333333">
  <meta name="description" content="这一切都是stein;gates的选择吧">
  <meta name="author" content="Imitators">
  <meta name="keywords" content="">
  <meta name="description" content="\[ \newcommand{\E}{\mathbb{E}} \] Sketch Supervised Learning-Part2  SVM Decision Tree Random forest &amp; Boosting   SVM SVM(hard margin)  我们希望找到一个直线（超平面）：\(\lang w,x\rang-b&#x3D;0\)，使得他将集合">
<meta property="og:type" content="website">
<meta property="og:title" content="Supervised Learning-Part2">
<meta property="og:url" content="https://proton-z.github.io/ML/SupervisedLearningPart2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="\[ \newcommand{\E}{\mathbb{E}} \] Sketch Supervised Learning-Part2  SVM Decision Tree Random forest &amp; Boosting   SVM SVM(hard margin)  我们希望找到一个直线（超平面）：\(\lang w,x\rang-b&#x3D;0\)，使得他将集合">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20241231164732334.png">
<meta property="article:published_time" content="2024-12-18T16:00:00.000Z">
<meta property="article:modified_time" content="2025-01-01T09:20:37.413Z">
<meta property="article:author" content="Imitators">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20241231164732334.png">
  
  <title>Supervised Learning-Part2 - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/color-brewer.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"proton-z.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>My blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/proton-z/cdn@2.0/2233/a33.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Supervised Learning-Part2">
              
            </span>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-x-md">
        <div class="py-5" id="board"
          >
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                

<article class="page-content">
  <p><span class="math display">\[
\newcommand{\E}{\mathbb{E}}
\]</span></p>
<h2 id="sketch">Sketch</h2>
<p><a href="">Supervised Learning-Part2</a></p>
<ul>
<li>SVM</li>
<li>Decision Tree</li>
<li>Random forest &amp; Boosting</li>
</ul>
<hr />
<h1 id="svm">SVM</h1>
<h2 id="svmhard-margin">SVM(hard margin)</h2>
<ul>
<li><p>我们希望找到一个直线（超平面）：<span class="math inline">\(\lang
w,x\rang-b=0\)</span>，使得他将集合 <span
class="math inline">\(S\)</span> 划分成两半 <span
class="math inline">\(S_1\)</span>，<span
class="math inline">\(S_2\)</span> 使得 label 正确。</p></li>
<li><p>同时我们希望这个直线拥有最大的margin，因为直线的形式是 <span
class="math inline">\(\langle w,x\rangle-b=0\)</span>, <span
class="math inline">\(w\)</span> 是直线的法向量，也就是希望 <span
class="math inline">\(\min_{i}|\langle w,x_i\rangle-b|\)</span>
最大。而那些在边界上的点，被我们称为 support vector。而这里的 <span
class="math inline">\(\ell_2\)</span>-norm 长度为：<span
class="math inline">\(\frac{1}{\| w\|_2}\)</span>。</p></li>
<li><p>数据点为：<span class="math inline">\((x_i,y_i)\)</span>, <span
class="math inline">\(y_i\in\{1,-1\}\)</span>，那么</p>
<ul>
<li><span class="math inline">\(y_i(\langle w,x_i\rangle-b)\ge
1\)</span>。这个可以用 二次规划polynomial时间内解决。</li>
</ul></li>
</ul>
<hr />
<h2 id="svmsoft-margin">SVM(soft margin)</h2>
<p>Hinge Loss: 我们把误差看成（此时暂时忽略掉 <span
class="math inline">\(b\)</span>）： <span class="math display">\[
\max(0,1-y_i\langle w,x\rangle)
\]</span> 那么最优化的可以看成： <span class="math display">\[
\begin{aligned}
\min\|w\|_2+\lambda\sum_{}\max(0,1-y_i\langle w,x\rangle)\\
\end{aligned}
\]</span> 我们可以添加松弛变量（<span
class="math inline">\(\xi_i\)</span>）： <span class="math display">\[
\begin{aligned}
\min\| w\|_2+\lambda\sum \xi_i\\
s.t. y_i\langle w,x\rangle\ge 1-\xi_i,\xi_i\ge 0
\end{aligned}
\]</span></p>
<h2 id="duality">Duality</h2>
<p><span class="math display">\[
\begin{aligned}
(1)\ \ &amp;\min f(x),s.t.\  c_i(x)\ge 0\\
(2)\ \ &amp;\max_{\alpha\ge 0}(\min_{x} f(x)-\sum \alpha_ic(x))
\end{aligned}
\]</span></p>
<p>假设 <span class="math inline">\((1)\)</span> 的最小值为 <span
class="math inline">\(d^*\)</span>，而 <span
class="math inline">\((2)\)</span> 的最小值为 <span
class="math inline">\(p^*\)</span>，那么我们有 <span
class="math inline">\(d^*\ge p^*\)</span>。</p>
<h3 id="proof">proof</h3>
<p>我们先固定 <span class="math inline">\(\alpha\)</span>
的取值，我们直接选取 <span class="math inline">\(x^*\)</span> 也就是
<span class="math inline">\((1)\)</span> 的最小值，可以发现，<span
class="math inline">\(c_i(x)\ge 0\)</span>，那么当前的答案一定 <span
class="math inline">\(\le d^*\)</span>。故 <span
class="math inline">\(p^*\le d^*\)</span>。这个性质被我们成为
duality。</p>
<h2 id="ktt-condition">KTT-condition</h2>
<p>如果 <span class="math inline">\(p^*=d^*\)</span> 那我们称其为 strong
duality。</p>
<ol type="1">
<li><p>如果 <span class="math inline">\(\alpha_i\not=0\)</span> 那说明
<span class="math inline">\(c_i(x)=0\)</span>，否则 <span
class="math inline">\(\alpha_i=0\)</span> 说明 <span
class="math inline">\(c_i(x)\ge0\)</span> 不紧。</p>
<p>那么我们能得到 strong duality 的一条性质： <span
class="math inline">\(\langle\alpha^*,c(x^*)\rangle=0\)</span>。</p></li>
<li><p><span class="math inline">\(\nabla_x
L(x^*,\alpha^*)=0\)</span>。关于 <span
class="math inline">\(x^*\)</span> 是 <span
class="math inline">\(\nabla=0\)</span>
也是好理解的，因为他是内层极值点。关于 <span
class="math inline">\(\alpha_i\)</span> 那侧可能存在导数 <span
class="math inline">\(\not=0\)</span> 的方向，因为有 <span
class="math inline">\(\alpha_i\ge0\)</span> 的限制。</p></li>
</ol>
<hr />
<h3 id="dual-svm">Dual SVM</h3>
<p><span class="math display">\[
\begin{aligned}
&amp;\min\frac{1}{2}\| w\|_2^2\\
s.t.\ &amp;\forall i,y_i(\langle w,x_i\rangle+b)\ge 1\\
\end{aligned}
\]</span></p>
<p>Dual 问题： <span class="math display">\[
\begin{aligned}
L(w,\alpha)=\frac{1}{2}\| w\|_2^2-\sum \alpha_i(y_i(\langle
w,x_i\rangle+b)-1)\\
\max_{\alpha}\min_wL(w,\alpha)\ s.t\ \alpha\ge 0\\
\end{aligned}
\]</span> 那么此时 gradient 条件即为： <span class="math display">\[
\begin{aligned}\
\nabla_wL(w,\alpha)=w-\sum\alpha_iy_ix_i=0&amp;\implies
w=\sum\alpha_iy_ix_i\\
\nabla_b L=\sum\alpha_i y_i
\end{aligned}
\]</span> 然后我就可以解决只关于 <span
class="math inline">\(\alpha\)</span> 的二次规划即可，即： <span
class="math display">\[
\begin{aligned}
&amp;\frac{1}{2}\sum_{i,j}\langle
\alpha_iy_ix_i,\alpha_jy_jx_j\rangle-\sum_{i}\alpha_i\left(y_i\left\langle
x_i,\sum_{j}\alpha_jy_jx_j\right\rangle-1\right)\\
=&amp;\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\langle
x_i,x_j\rangle-\sum_{i,j}\alpha_i\left(y_i\left\langle
x_i,\alpha_jy_jx_j\right\rangle-1\right)\\
=&amp;\sum_{i=1}\alpha_i-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\langle
x_i,x_j\rangle
\end{aligned}
\]</span></p>
<p>其中 <span class="math inline">\(\alpha_i\ge 0,\sum\alpha_i
y_i=0\)</span>。</p>
<hr />
<h2 id="relax-svm">relax SVM</h2>
<p><span class="math display">\[
\begin{aligned}
\min_{w,\lambda,\xi}\frac{1}{2}\|w\|_2^2+\lambda\sum_{} \xi_i\\
s.t.\ y_i(\langle w,x_i+b)\rangle\ge 1-\xi_i,\xi_i\ge0
\end{aligned}
\]</span></p>
<p>Dual 问题： <span class="math display">\[
\begin{aligned}
L(w,\lambda,\xi,\alpha,\kappa)&amp;=\frac{1}{2}\| w\|_2^2+\lambda\sum
\xi_i-\sum \alpha_i(y_i(\langle
w,x_i\rangle+b)-1+\xi_i)-\sum\kappa_i\xi_i\\
s.t.&amp;\ \alpha_i\ge0,\kappa_i\ge 0
\end{aligned}
\]</span> 我们计算梯度： <span class="math display">\[
\begin{aligned}
&amp;\nabla_wL=w-\sum \alpha_iy_ix_i\\
&amp;\nabla_{\xi_i}L=\lambda-\alpha_i-\kappa_i\\
&amp;\nabla_b L=\sum \alpha_iy_i
\end{aligned}
\]</span></p>
<p>也就是： <span class="math display">\[
\begin{aligned}
w&amp;=\sum_{}\alpha_iy_ix_i;0\le\alpha_i\le\lambda;\sum\alpha_iy_i=0\\
L(w^*,\lambda,\xi,\alpha,\kappa)&amp;=\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\langle
x_i,x_j\rangle-\sum\alpha_iy_i\langle
x_i,w\rangle+\sum\alpha_i(1-\xi_i)-\sum\kappa_i\xi_i+\lambda\sum \xi_i\\
&amp;=\sum\alpha_i-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\langle
x_i,x_j\rangle
\end{aligned}
\]</span> (notice: <span
class="math inline">\(\lambda-\alpha_i-\kappa_i=0\)</span>) 而其中 <span
class="math inline">\(\sum \alpha_iy_i=0\)</span>, <span
class="math inline">\(0\le \alpha_i\le \lambda\)</span>。</p>
<hr />
<h3 id="kernel-method">kernel method</h3>
<p>我们把低维，非线性可分的东西映射到高维。希望在高维线性可分。</p>
<p>找到 kernel function: <span class="math inline">\(\phi\)</span>。</p>
<p>那么目的是找到高维中的向量 <span
class="math inline">\(w\)</span>，s.t. <span class="math display">\[
\begin{aligned}
&amp;\min_{w}\frac{1}{2}\| w\|_2^2+\lambda\sum \xi_i\\
s.t.\ &amp;
y_i(\langle \phi(x_i),w\rangle+b)\ge 1-\xi_i,\xi_i\ge 0\end{aligned}
\]</span> 得出 duality: <span class="math display">\[
\begin{aligned}
&amp;\sum_{i}\alpha_i-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_j\langle\phi(x_i),\phi(x_j)\rangle\\
s.t.\ &amp; 0\le \alpha_i\le \lambda,\sum \alpha_iy_i=0
\end{aligned}
\]</span> <span
class="math inline">\(w=\sum\alpha_iy_i\phi(x_i)\)</span>。</p>
<p>至此，我们不需要一个具体的 <span
class="math inline">\(\phi(x_i)\)</span>，而是需要 <span
class="math inline">\(\langle \phi(x_i),\phi(y_i)\rangle\)</span>
内积组即可。</p>
<p>因为我们给定一个新 <span
class="math inline">\(x_0\)</span>，只需要判断：<span
class="math inline">\(\langle w,\phi(x_0)\rangle=\sum\alpha_iy_i\langle
\phi(x_i),\phi(x_0)\rangle\)</span>。</p>
<h3 id="mercers-theorem">mercer's theorem</h3>
<p>给定一个半正定对称阵 <span class="math inline">\(K\)</span>
存在一个映射 <span class="math inline">\(\phi\)</span> 使得 <span
class="math inline">\(K(x_i,x_j)=\langle
\phi(x_i),\phi(x_j)\rangle\)</span>。</p>
<p>Gaussian kernel: <span
class="math inline">\(K(x_i,x_j)=\exp\left(-\frac{\|
x_i-x_j\|^2}{2\sigma^2}\right)\)</span>。</p>
<hr />
<h1 id="decision-tree">decision tree</h1>
<ol type="1">
<li>boolean functional analysis</li>
<li>Gini Index</li>
<li>Random forest</li>
<li>Boosting</li>
</ol>
<h2 id="boolean-functional-analysis">Boolean functional analysis</h2>
<p><span class="math inline">\(f:\{0,1\}^n\to
\mathbb{R}\)</span>，这样的函数，我们可以用 <span
class="math inline">\(2^n\)</span> 个 <span
class="math inline">\(f(S)\)</span>
直接表示，也可以换一种方式，用傅里叶basis 表示。</p>
<p>Notation: <span class="math inline">\(D\)</span> 表示 <span
class="math inline">\(\{0,1\}^n\)</span> uniform distribution. 由于
<span class="math inline">\(\{-1,1\}\)</span> 的均值是 <span
class="math inline">\(0\)</span>，我们也可以用 <span
class="math inline">\(\{-1,1\}^n\)</span> 表示。</p>
<ul>
<li>我们用 <span class="math inline">\(\E_{x\sim D}|f(x)-g(x)|^2\le
\epsilon\)</span> 来描述两个函数比较接近。</li>
</ul>
<h3 id="fourier-basis">Fourier basis</h3>
<p><span class="math inline">\(\chi_s=\prod_{i\in
s}x_i\)</span>，我们可以发现 <span class="math inline">\(\chi_s\)</span>
是 boolean functional 的一个 orthonormal basis. <span
class="math display">\[
\begin{aligned}
\langle\chi_s,\chi_s\rangle=\frac{1}{2^n}\sum_{u\subseteq
\{-1,1\}^n}\chi_s(u)\chi_s(u)=\frac{1}{2^n}\sum_{u\subseteq\{-1,1\}^n}1=1\\
\langle
\chi_s,\chi_t\rangle=\frac{1}{2^n}\sum_{u\subseteq\{-1,1\}^n}\chi_s(u)\chi_t(u)=\frac{1}{2}\sum_{u\subseteq\{-1,1\}^n}\chi_{s\oplus
t}(u)=[s=t]
\end{aligned}
\]</span> basis 是考虑 dimension。</p>
<p>所以： <span class="math display">\[
\begin{aligned}
f=\sum_{s\subseteq[n]}\langle\chi_s,f\rangle\chi_s,\hat {f_{s}}=\langle
\chi_s,f\rangle
\end{aligned}
\]</span> 我们继续定义在 Fourier basis 上的 <span
class="math inline">\(L_1\)</span>-norm 和 sparsity. <span
class="math display">\[
\begin{aligned}
{\mid f\mid}_1=\sum_{s\subseteq[n]} |\hat {f_s}|,|f|_0=\sum_{s\subseteq
[n]}[\hat {f_{s}}\not=0]\\
\end{aligned}
\]</span> Parseval Identity: <span class="math display">\[
\begin{aligned}
\langle f,f\rangle=\sum_{s\subseteq[n]} \hat{f_s}^2
\end{aligned}
\]</span></p>
<hr />
<h2 id="decision-tree---polynomial">decision tree -&gt; polynomial</h2>
<p>我们希望将 decision tree 用low degree, sparsity polynomial
表示出来。</p>
<p>注意，我们有假设，我们假设在 decision tree 上面随机往左右走（每次分叉
1/2 概率）。</p>
<p>误差是：<span class="math inline">\(\E_{x\sim
D}[f(x)\not=g(x)]\)</span> 其中 <span class="math inline">\(f\)</span>
是你的估计， <span class="math inline">\(g\)</span> 是正确估计。</p>
<p>此处的 decision tree 是叶子值是 <span
class="math inline">\(\pm1\)</span> 的 decision tree。</p>
<p><img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20241231164732334.png" srcset="/img/loading.gif" lazyload alt="image-20241231164732334" style="zoom:50%;" /></p>
<h3 id="theorem">Theorem</h3>
<p>对于一个有 <span class="math inline">\(s\)</span>
个叶子的决策树，存在一个度数为 <span
class="math inline">\(\log(\frac{s}{\epsilon})\)</span> 且 <span
class="math inline">\(\frac{s^2}{\epsilon}\)</span>-sparsity 的
polynomial <span class="math inline">\(f\)</span> 使得误差最多为 <span
class="math inline">\(4\epsilon\)</span>。</p>
<ul>
<li><p>step 1. 将深度大于 <span
class="math inline">\(\log(\frac{s}{\epsilon})\)</span>
的叶子全部割掉，那么每一个深于 <span
class="math inline">\(\log(\frac{s}{\epsilon})\)</span>
的叶子贡献权值：<span
class="math inline">\((1/2)^{\log(s/\epsilon)}=\frac{\epsilon}{s}\)</span>。<span
class="math inline">\(\E[(f-g)^2]\le \frac{\epsilon}{s}\times
s=\epsilon\)</span></p></li>
<li><p>step 2. 一个有 <span class="math inline">\(s\)</span> 个叶子的
decision tree 如果树高不超过 <span
class="math inline">\(h:=\log(s/\epsilon)\)</span> 的话，那他能被表示成
一个 <span class="math inline">\(L_1(f)\le s\)</span> 的 <span
class="math inline">\(h\)</span> degree 的 decision tree.（每一个叶子用
AND 连起来）</p></li>
<li><p>step 3. 如果 <span class="math inline">\(f\)</span> 满足 <span
class="math inline">\(L_1(f)\le s\)</span>,<span
class="math inline">\(\deg f\le \log(s/\epsilon)\)</span>。则存在一个
<span class="math inline">\(\E[(f-g)^2]\le\epsilon\)</span> 的 <span
class="math inline">\(g\)</span> 使得，<span
class="math inline">\(L_1(g)\le s,L_0(g)\le
\frac{s^2}{\epsilon}\)</span> 。</p>
<p>保留 <span class="math inline">\(|\hat f_s|\ge
\frac{\epsilon}{L_1(f)}\)</span> ，至多保留 <span
class="math inline">\(\frac{L_1(f)^2}{\epsilon}\)</span> 个，我们用
parseval identity 可以说明：x</p>
<p><span class="math display">\[
\begin{aligned}
|f-g|^2=\sum_{\hat {f_s}&lt;\frac{\epsilon}{L_1(f)}} \hat f_s^2\le
\max_{s}\hat{f_s}\sum_{s}\hat{f_s}\le\frac{\epsilon}{L_1(f)}L_1(f)=\epsilon
\end{aligned}
\]</span></p></li>
</ul>
<hr />
<p>综上，最开始的为 <span class="math inline">\(f\)</span>，砍掉叶子的是
<span class="math inline">\(g\)</span>，最终的是 <span
class="math inline">\(h\)</span>。</p>
<p><span class="math inline">\(\Vert f-h\Vert^2\le 2\Vert
f-g\Vert^2+2\Vert g-h\Vert^2=4\epsilon\)</span>。</p>
<hr />
<p>我们只证明了存在性，可以使用 LMN 去 learn 这个 low degree sparsity 的
polynomial.</p>
<p>随机 sample <span class="math inline">\(m\)</span>
个点，然后我们令，其中 <span class="math inline">\(|s|\le
\log(s/\epsilon)\)</span>： <span class="math display">\[
\begin{aligned}
\hat{f_s}=\frac{1}{m}\sum_{i=1}^mf(x_i)\chi_{s}(x_i)\approx E_{x\sim
D}f(x)\chi_s(x)
\end{aligned}
\]</span> 需要做: <span
class="math inline">\(\tilde{O}(\frac{s^2}{\epsilon^2}\log n)\)</span>
次达到 <span class="math inline">\(\epsilon\)</span> 误差。（如果我们把
<span class="math inline">\(2^n\)</span> 个子集全部
sample，便得到了原函数）</p>
<ul>
<li>实际表现很差</li>
<li>噪声没有保证</li>
</ul>
<h3 id="compressed-sensing">compressed sensing</h3>
<p>random orthonormal basis: <span class="math display">\[
\begin{aligned}
\langle \psi_i,\psi_j\rangle=\E_{x\sim D}[\psi_i(x)\psi_j(x)]=[i=j]
\end{aligned}
\]</span> Fourier basis 自然是 random orthonormal basis。</p>
<p>Compressed sensing 想说的是，如果需要学习的向量很长，但是却有hidden
structure，我们可以设置一个好的矩阵 <span
class="math inline">\(A_{n,m}\)</span>, <span
class="math inline">\(n&lt;&lt;m\)</span> 来观测 <span
class="math inline">\(y=Ax\)</span> 的表现。</p>
<p>所以有定理：</p>
<p>如果有一个矩阵 <span class="math inline">\(A_{m\times n}\)</span>
其中每一行，第 <span class="math inline">\(i\)</span> 行表示 <span
class="math inline">\(\psi_0(x_i),\cdots,\psi_s(x_i),\cdots\)</span>，我们随机选点
<span class="math inline">\(x_i\)</span>，得到观测 <span
class="math inline">\(y=Ax+e\)</span>, <span class="math inline">\(x\in
\mathbb{R}^n\)</span> 是 <span
class="math inline">\(s\)</span>-sparsity。然后我们希望用 LASSO
去还原这个 <span class="math inline">\(y\)</span>。假设我们采样了 <span
class="math inline">\(m\ge \tilde{O}(s\log n)\)</span>，有 <span
class="math inline">\(\ge 1-\delta\)</span> 概率： <span
class="math display">\[
\begin{aligned}
\| x-x^*\|_2\le c\frac{\|e\|_2}{\sqrt m}
\end{aligned}
\]</span> 如果我们能使 <span class="math inline">\(c\frac{\|
e\|_2}{\sqrt m}\le \epsilon\)</span> ，就能达到一个 <span
class="math inline">\(\epsilon\)</span>-error 的 <span
class="math inline">\(f\)</span>。</p>
<hr />
<h2 id="gini-index">Gini index</h2>
<p>有一属性 <span class="math inline">\(A/B\)</span>，还有一个二分类结果
<span class="math inline">\(0/1\)</span>，我们定义 Gini 系数： <span
class="math display">\[
\begin{aligned}
\sum_{i\in\{A,B\}}\mathrm{Pr}[c=i]\times(1-\mathrm{Pr}[r=0|c=i]^2-\mathrm{Pr}[r=1|c=i]^2)
\end{aligned}
\]</span> 选取最小 Gini index 对应的属性。</p>
<p>Gini index 越小，预示着预测越准。(<span class="math inline">\(1-\sum
p_i^2\)</span> 最小值取在 <span
class="math inline">\((1,0,0,\cdots)\)</span> 处)</p>
<hr />
<h2 id="random-forest">random forest</h2>
<p>决策树很容易就 overfit。</p>
<p>假设训练数据是 <span class="math inline">\((x_i,y_i)\)</span>，<span
class="math inline">\(1\le i\le n\)</span>。那么我们每次可重的 sample
<span class="math inline">\(n\)</span> 次，假设选出来的数据集是 <span
class="math inline">\(X_i,Y_i\)</span>。</p>
<p>我们可以知道一个数据点没被选上的概率是 <span
class="math inline">\((1-\frac{1}{n})^n\approx
\frac{1}{e}\)</span>。着有效的减弱了 overfitting 的影响。</p>
<p>然后我没对每一个 <span class="math inline">\(X_i,Y_i，1\le i\le
B\)</span> 训练出一个决策树：<span
class="math inline">\(T_i\)</span>，最终我们将这些决策树的结果取均值：
<span class="math display">\[
\begin{aligned}
\overline{T}(x)=\frac{1}{m}\sum_{i=1}^mT_i(x)
\end{aligned}
\]</span></p>
<h3 id="feature-bagging">feature bagging</h3>
<p>我们不光可以对数据集合进行 bagging，我们还可以对数据点本身（feature）
bagging。</p>
<p>比如说原先一个 <span class="math inline">\(n\)</span>-dimension
的向量，我只选 <span class="math inline">\(d\)</span>
维，mask掉其他维，这样可以避免对一些“重要信息” overfitting。</p>
<hr />
<h1 id="boosting">Boosting</h1>
<h2 id="stacked-learner">Stacked learner</h2>
<p>Very Weak.</p>
<p>有若干 weak learner <span
class="math inline">\(h_i(x)\)</span>，我们构造一个新数据集：<span
class="math inline">\((x_i&#39;,y_i)=((h_1(x_i),\cdots
h_n(x_i)),y_i)\)</span>，我们希望学习这个 <span
class="math inline">\((X&#39;,Y)\)</span>
这个数据集。（可以是取平均数，中位数......） 总之，效果很差。</p>
<h2 id="adaboost">Adaboost</h2>
<p>framework:</p>
<ol type="1">
<li><p>假设数据集是 <span
class="math inline">\((x_1,y_1),\cdots,(x_m,y_m)\)</span>，依旧假设是二分类：<span
class="math inline">\(y_i\in\{-1,1\}\)</span>。</p></li>
<li><p>初始构造一个均匀分布 <span
class="math inline">\(D_1(x_i,y_i)=\frac{1}{m}\)</span>，每次根据分布训练出一个模型
<span class="math inline">\(h_i\)</span>。</p></li>
<li><p>怎么根据 <span
class="math inline">\(D_t,h_t\)</span>，构造出新的分布？ <span
class="math display">\[
\begin{aligned}
\alpha_t=\frac{1}{2}\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right),\epsilon_t=Loss(h_t)\\
D_{t+1}(x_i)=\frac{D_t(x_i)}{z_t}\exp(-\alpha_ty_ih_t(x_i))
\end{aligned}
\]</span> Final classifier 将会是： <span
class="math inline">\(h_{\mathrm{final}}(\cdot)=\mathrm{sign}\left(\sum_{t}\alpha_th_t(\cdot)\right)\)</span>
。</p></li>
</ol>
<hr />
<h2 id="theorem-1">Theorem</h2>
<p>假设 <span
class="math inline">\(\gamma_t=\frac{1}{2}-\epsilon_t\)</span>，那么则有：
<span class="math display">\[
\begin{aligned}
L(h_{\mathrm{final}})\le
\prod_{t}2\sqrt{\epsilon\left(1-\epsilon\right)}=\prod_{t}\sqrt{1-4\gamma_t^2}\le
\exp\left(-2\sum\gamma_t^2\right)
\end{aligned}
\]</span> 假设我们可以估计 <span class="math inline">\(\gamma_t\ge
\gamma\)</span> 的下界 <span
class="math inline">\(\gamma\)</span>，那么则有： <span
class="math display">\[
\begin{aligned}
L(h_{\mathrm{final}})\le \exp(-2\gamma^2T)
\end{aligned}
\]</span> 可以发现，这是一个自适应(adaptive)的，因为我们不需要设置 <span
class="math inline">\(\gamma\)</span> 与 <span
class="math inline">\(T\)</span>。</p>
<h3 id="step-1">step 1</h3>
<p><span class="math display">\[
\begin{aligned}
D_{T}(x_i)&amp;=\frac{D_{T-1}(x_i)}{z_{T-1}}\exp(-\alpha_ty_ih_t(x_i))\\
&amp;=\frac{1}{m}\frac{\exp(-\sum_t\alpha_ty_ih_t(x_i))}{\prod_{t}z_t}=\frac{1}{m}\frac{\exp(-y_if(x_i))}{\prod_{t}z_t}
\end{aligned}
\]</span></p>
<p>where <span
class="math inline">\(f(x_i):=\sum_{t}\alpha_th_t(x_i)\)</span>。</p>
<hr />
<h3 id="step-2">step 2</h3>
<p><span class="math inline">\(L(h_{\mathrm{final}})\le \prod_t
z_t\)</span>。 <span class="math display">\[
\begin{aligned}
L(h_{\mathrm{final}})&amp;=\frac{1}{m}\sum_{i=1}^m[y_i\not=h_{\mathrm{final}}(x_i)]=\frac{1}{m}\sum_{i=1}^m[y_if(x_i)\le
0]\\
&amp;\le\frac{1}{m}\sum_{i=1}^m\exp(-y_if(x_i))=\frac{1}{m}\sum_{i=1}^mm\prod_{t}z_tD_T(x_i)=\prod_{t}z_t
\end{aligned}
\]</span></p>
<hr />
<h3 id="step-3">step 3</h3>
<p><span class="math display">\[
\begin{aligned}
z_t&amp;=\sum_{i=1}^mD_t(x_i)\exp(-\alpha_ty_ih_t(x_i))\\
&amp;=\sum_{y_i=h_t(x_i)}D_t(x_i)\exp(-\alpha_t)+\sum_{y_i\not=h_t(x_i)}
D_t(x_i)\exp(\alpha_t)\\
&amp;=(1-\epsilon_t)\exp(-\alpha_t)+\epsilon_t\exp(\alpha_t)
\end{aligned}
\]</span></p>
<p>minimizer is: <span
class="math inline">\((1-\epsilon_t)\exp(-\alpha_t)=\epsilon_t\exp(\alpha_t)\)</span>,
<span
class="math inline">\(\alpha_t=\frac{1}{2}\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)\)</span>。
<span class="math display">\[
\begin{aligned}
z_t=2\sqrt{(1-\epsilon_t)\epsilon_t}&amp;\\&amp;\ \ \ \ \square
\end{aligned}
\]</span></p>
<hr />
<p>Adaptive 会不会随着 <span class="math inline">\(T\)</span> 增大，导致
test case 过拟合，而 population loss 反而增大？</p>
<p>事实上，对 adaboost 来说这并不会，因为一下两个定理保证：</p>
<h3 id="theorem-1-1">theorem 1</h3>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathrm{Pr}_{(x,y)\sim S}[yf(x)\le \theta]\\
\mathrm{Pr}_{(x,y)\sim D}[yf(x)\le 0]&amp;\le \mathrm{Pr}_{(x,y)\sim
S}[yf(x)\le \theta]+O\left(\frac{1}{\sqrt{m}}\left(\frac{\log
m\log|H|}{\theta^2}+\log\frac{1}{\delta}\right)^{\frac{1}{2}}\right)
\end{aligned}
\]</span></p>
<p>其中 <span class="math inline">\(S\)</span> 是一个依照分布 <span
class="math inline">\(D\)</span> 选取的大小是 <span
class="math inline">\(m\)</span> 的数据集，<span
class="math inline">\(H\)</span> 是有限的 hypothesis set,然后就会有
<span class="math inline">\(1-\delta\)</span>
的概率满足上面那个式子。</p>
<p>并未证明。</p>
<h3 id="theorem-2">theorem 2</h3>
<p>我们将 <span class="math inline">\(h_{\mathrm{final}}\)</span> 的定义
slightly 更改一下： <span class="math display">\[
\begin{aligned}
f(x)=\frac{\sum_{t}\alpha_th_t(x)}{\sum\alpha_t}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{Pr}_{(x,y)\sim S}[yf(x)\le\theta]\le
\prod_{t=1}^m2\sqrt{(1-\epsilon_t)^{1+\theta}\epsilon_t^{1-\theta}}
\end{aligned}
\]</span></p>
<p>Proof: <span class="math display">\[
\begin{aligned}
yf(x)\le \theta&amp;\iff y\frac{\sum_{t}\alpha_th_t(x)}{\sum\alpha_t}\le
\theta\iff y\sum_{t}\alpha_th_t(x)\le \theta\sum_{t}\alpha_t\\
\mathrm{Pr}_{(x,y)\sim S}[yf(x)\le \theta]&amp;=\mathrm{Pr}_{(x,y)\sim
S}\left[\exp\left(\theta\sum_{t}\alpha_t-y\sum_{t}\alpha_th_t(x)\right)\ge1\right]\\
&amp;=\E_{(x,y)\sim
S}\left[\exp\left(\theta\sum_{t}\alpha_t-y\sum_{t}\alpha_th_t(x)\right)\right]\\
&amp;=\exp\left(\theta\sum_{t}\alpha_t\right)\E_{(x,y)\sim
S}\left[\exp\left(-y\sum_{t}\alpha_th_t(x)\right)\right]\\
&amp;=\exp\left(\theta\sum_{t}\alpha_t\right)\frac{1}{m}\sum_{i=1}^m\left[\exp\left(-y_i\sum_{t}\alpha_th_t(x_i)\right)\right]\\
&amp;=\exp\left(\theta\sum_{t}\alpha_t\right)\frac{1}{m}m\prod_{t=1}^mz_t\\
&amp;=\exp\left(\theta\sum_{t}\alpha_t\right)\prod_{t=1}^m2\sqrt{\epsilon_t(1-\epsilon_t)}\\
\end{aligned}
\]</span> 公式太长，小歇一下，<span
class="math inline">\(\alpha_t:=\frac{1}{2}\log\left(\frac{1-\epsilon_t}{\epsilon_t}\right)\)</span>。
<span class="math display">\[
\begin{aligned}
\mathrm{LHS}&amp;=\prod_{t=1}^m2\sqrt{\frac{(1-\epsilon_t)^\theta}{\epsilon_t^\theta}}\sqrt{\epsilon_t(1-\epsilon_t)}\\
&amp;=\prod_{t=1}^m2\sqrt{(1-\epsilon_t)^{1+\theta}\epsilon_t^{1-\theta}}\\
&amp;=\prod_{t=1}^m\sqrt{(1+2\gamma_t)^{1+\theta}(1-2\gamma_t)^{1-\theta}}\\
&amp;\le\sqrt{(1+2\gamma)^{1+\theta}(1-2\gamma)^{1-\theta}}^m
\end{aligned}
\]</span> when <span class="math inline">\(\theta&lt;\gamma\)</span>
时，我们能得出 <span class="math inline">\(\sqrt{.}\)</span> 是 <span
class="math inline">\(&lt; 1\)</span> 的，所以 <span
class="math inline">\(m\)</span> 会指数衰减。</p>
<p>为什么？设 <span
class="math inline">\(\theta=k\gamma\)</span>，考虑看成 <span
class="math inline">\(\gamma,k\)</span> 的函数，求一下导，发现 <span
class="math inline">\(k\)</span> 越大函数值越大，并且我们可以证明 <span
class="math inline">\(k=1\)</span> 时合法。</p>
<hr />
<h2 id="gradient-boost">gradient boost</h2>
<p>事实上 adaboost 可以看成一种 coordinate descent。 (每次只选取
gradient 的一维进行更改)（why?空间）</p>
<p>我们先看一种 linear regression 的 boosting。我们假设 Loss function
为：<span
class="math inline">\(L(f(x),y)=\sum_{i=1}^m\frac{\|f(x_i)-y_i\|^2}{2}\)</span>，square
loss。 <span class="math display">\[
\begin{aligned}
y_i\approx f(x_i)\\
\end{aligned}
\]</span> 我们新建一个数据集：<span
class="math inline">\((x_i,y_i-f(x_i))\)</span>，然后训练一个新
regression tree <span
class="math inline">\(h\)</span>。非常符合直觉。</p>
<p>但他究竟在干什么呢？实际上，这是在学习梯度。</p>
<p>我们把 <span class="math inline">\(f(x_i)\)</span> 看成一个变元：
<span class="math display">\[
\begin{aligned}
\nabla_{f(x_i)}L=f(x_i)-y
\end{aligned}
\]</span> 实际上就是在做梯度下降。如果 Loss function
改变，我们新建立的集合应为： <span class="math inline">\((x,-\nabla
L_x)\)</span>。</p>
<p>比如说 绝对值函数 作为 loss function(<span
class="math inline">\(\sum_{}|f(x_i)-y_i|\)</span>)，我们选择的新集合就应为：<span
class="math inline">\((x_i,\mathrm{sign}(f(x_i)-y_i))\)</span>。</p>
<hr />
<p>为什么 adaboost 可以看作一种 coordinate descent？</p>
<p>考虑 adaboost 实际上是在最小化： <span class="math display">\[
\begin{aligned}
\sum_{i=1}^m\exp(-y_if(x_i))
\end{aligned}
\]</span> 其中： <span class="math display">\[
\begin{aligned}
f(x)=\sum_{t=1}^T\alpha_tf_t(x)
\end{aligned}
\]</span> 我们可以看成，现在我们 hypothesis set 里面有 <span
class="math inline">\(N\)</span> 个 weak learner <span
class="math inline">\(g_1,\cdots,g_N\)</span>，我们的 <span
class="math inline">\(f\)</span> 是这些 weak learner 的 combination。
<span class="math display">\[
\begin{aligned}
f(x)=\sum_{j=1}^N\lambda_j g_j(x)
\end{aligned}
\]</span> 最初，我们的 <span
class="math inline">\(\lambda_j=0\)</span>，每次选出（根据概率分布 <span
class="math inline">\(D_{t+1}\)</span> 求出）对应的 <span
class="math inline">\(g_{t+1}\)</span> 使得之前的 <span
class="math inline">\(\lambda_{t+1}=0\)</span>，现在我们进行赋值：<span
class="math inline">\(\lambda_{t+1}=\alpha_{t+1}\)</span>。</p>
<p>为何选择 <span class="math inline">\(\alpha_{t+1}\)</span>，因为这是
minimizer（方向导数）。 <span class="math display">\[
\begin{aligned}
L(\lambda_1,\cdots\lambda_N)&amp;=\sum_{i=1}^m\exp\left(-y_i\sum_{t=1}^T\lambda_tg_t(x_i)\right)\\
\nabla_{\lambda_s}L&amp;=\sum_{i=1}^m\exp\left(-y_i\sum_{t=1,t\not=s}^T\lambda_tg_t(x_i)\right)\nabla\exp(-y_i\lambda_sg_s(x_i))\\
&amp;=m\prod_tz_t\sum_{i=1}^m-y_ig_s(x_i)\cdot\exp(-y_i\lambda_sg_s(x_i))\\
&amp;=m\prod_tz_t\left(\sum_{g_s(x_i)=y_i}^m-\exp(-\lambda_s)+\sum_{g_s(x_i)\not=y_i}^m\exp(\lambda_s)\right)\\
0&amp;=\sum_{g_s(x_i)=y_i}^m-\exp(-\lambda_s)+\sum_{g_s(x_i)\not=y_i}^m\exp(\lambda_s)\\
&amp;=-(1-\epsilon)\exp(-\lambda_s)+\epsilon\exp(\lambda_s)\\
\implies&amp;\exp(2\lambda_s)=\frac{1-\epsilon}{\epsilon},\lambda_s=\frac{1}{2}\ln\left(\frac{1-\epsilon}{\epsilon}\right)
\end{aligned}
\]</span></p>


  
</article>

              </div>
            </div>
          </div>
        </div>
      </div>
    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
