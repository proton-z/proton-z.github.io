

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#333333">
  <meta name="description" content="这一切都是stein;gates的选择吧">
  <meta name="author" content="Imitators">
  <meta name="keywords" content="">
  <meta name="description" content="\[ \newcommand{\Pr}{\mathrm{Pr}} \newcommand{\E}{\mathbb{E}} \] Other Stuff Other stuff  Robust machine learning  Obfuscated gradients Certified robustness  Hyperparameter tuning  (Bayes">
<meta property="og:type" content="website">
<meta property="og:title" content="Other Stuff">
<meta property="og:url" content="https://proton-z.github.io/ML/OtherStuff/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="\[ \newcommand{\Pr}{\mathrm{Pr}} \newcommand{\E}{\mathbb{E}} \] Other Stuff Other stuff  Robust machine learning  Obfuscated gradients Certified robustness  Hyperparameter tuning  (Bayes">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20241228160939956.png">
<meta property="og:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20250102005517290.png">
<meta property="og:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20250102015028509.png">
<meta property="article:published_time" content="2024-12-18T16:00:00.000Z">
<meta property="article:modified_time" content="2025-01-01T18:01:19.724Z">
<meta property="article:author" content="Imitators">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="c:/Users/zhang/AppData/Roaming/Typora/typora-user-images/image-20241228160939956.png">
  
  <title>Other Stuff - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/color-brewer.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"proton-z.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>My blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/proton-z/cdn@2.0/2233/a33.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Other Stuff">
              
            </span>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-x-md">
        <div class="py-5" id="board"
          >
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                

<article class="page-content">
  <p><span class="math display">\[
\newcommand{\Pr}{\mathrm{Pr}}
\newcommand{\E}{\mathbb{E}}
\]</span></p>
<h1 id="other-stuff">Other Stuff</h1>
<p>Other stuff</p>
<ul>
<li>Robust machine learning
<ul>
<li>Obfuscated gradients</li>
<li>Certified robustness</li>
</ul></li>
<li>Hyperparameter tuning
<ul>
<li>(Bayesian methods)</li>
<li>Gradient optimization</li>
<li>Random search</li>
<li>Multi-armed Bandits</li>
</ul></li>
</ul>
<hr />
<p>在图片上添加噪音可能使模型预测出的结果非常奇怪。</p>
<p>我们可以通过仅仅添加一个攻击照片，使得训练的结果异常错误。</p>
<hr />
<h2 id="adversarial-attacks-as-optimization">Adversarial attacks as
optimization</h2>
<p><span class="math display">\[
\begin{aligned}
\mathrm{E}_{x,y}[L(f_{\theta}(x),y)]\implies
\mathrm{E}_{x,y}\left[\max_{\delta\in\Delta}
L(f_{\theta}(x+\delta),y)\right]
\end{aligned}
\]</span></p>
<p>是考虑一个训练数据集 <span class="math inline">\((x,y)\)</span>
被人添加了 <span class="math inline">\(\delta\)</span>
的攻击最差能多差。</p>
<h2 id="projected-gradient-descent">projected gradient descent</h2>
<p>做法：正常做 GD，只不过每做一步之后将到达的点，project 回到限制域
<span class="math inline">\(\Delta\)</span> 上，<span
class="math inline">\(\delta_{t+1}=P_\Delta[\delta_t+\eta\nabla_x
L(f_{\theta}(x+\delta_t,y))]\)</span>。 为啥是 <span
class="math inline">\(\nabla_x\)</span> ，我们只不过相当于每次拽着 <span
class="math inline">\(x\)</span>
走一小步，使得它值提升的最多。所以应该朝着 <span
class="math inline">\(x\)</span> 提升值最多的放向走。</p>
<p>如果使用 L2-norm 作为 <span class="math inline">\(\Delta\)</span>
的话，可能会面临梯度噪音过大。</p>
<p>我们选择 <span class="math inline">\(L_\infty\)</span>-norm 作为
<span class="math inline">\(\Delta\)</span>，那么我们就有 lasso
类似的性质了。</p>
<h2 id="fast-gradient-sign-method">fast gradient sign method</h2>
<p>如果我们定义 <span
class="math inline">\(\Delta=\{\delta:\|\delta\|_{\infty}\le
\epsilon\}\)</span>, <span
class="math inline">\(P_{\Delta}(\delta)\)</span> 就是把 <span
class="math inline">\(\delta\)</span> 压缩到 <span
class="math inline">\((-\epsilon,\epsilon)\)</span>。</p>
<p>FGSM 只走一步，然后初始设置在 <span
class="math inline">\((0,\cdots,0)\)</span>
，我们假设学习率是无穷大的话，那么 projected gradient 将为： <span
class="math display">\[
\begin{aligned}
P_{\Delta}(\cdot)=\epsilon\cdot \mathrm{sign}(\nabla_xL(f_\theta(x),y))
\end{aligned}
\]</span> 另外一个类似的算法是 Projected gradient descent (PGD),我们将
FGSM 运行多次（迭代多次），并且设置 learning rate.
但是更加有可能得到一个更好的 local minimum.</p>
<hr />
<h2 id="adversarial-training">Adversarial training</h2>
<p>假设有人在攻击我们，我们的目标是最小化最坏情况的 loss: <span
class="math display">\[
\min_{\theta}\sum_{x,y\in
S}\max_{\delta\in\Delta}L(f_{\theta}(,x+\delta,y))
\]</span> 那么问题来了，我们该如何计算 <span
class="math inline">\(\phi(\theta)=\max_{\delta}
f(\theta,\delta)\)</span> 的 gradient呢？</p>
<h2 id="danskins-theorem">Danskins' Theorem</h2>
<p><span class="math display">\[
\begin{aligned}
\phi(\theta)&amp;:=\max_\delta f(\theta,\delta)\\
Z_0(\theta)&amp;:=\{\overline{\delta}:f(\theta,\overline{\delta})=\max_\delta
f(\theta,\delta\}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\phi(\theta)\)</span> 在 <span
class="math inline">\(\theta\)</span> 可微当且仅当 <span
class="math inline">\(Z_0(\theta)\)</span> 只有一个元素，设其为 <span
class="math inline">\(\delta^*\)</span>。</p>
<p>并且此时： <span class="math display">\[
\begin{aligned}
\frac{\partial \phi}{\partial \theta}=\frac{\partial
f(\theta,\delta^*)}{\partial \theta}
\end{aligned}
\]</span></p>
<hr />
<p>所以此时我们可以用： <span class="math display">\[
\begin{aligned}
\frac{\partial L(f_{\theta_t}(x+\delta^*),y)}{\partial \theta}
\end{aligned}
\]</span> 来计算梯度。</p>
<h2 id="adversarial-training-algorithm">Adversarial training
algorithm</h2>
<p>我们每次（每轮 <span
class="math inline">\(t\)</span>）只选取数据集的一个子集(a minibatch)
<span class="math inline">\(B\)</span> 去对这个 <span
class="math inline">\(B\)</span> attack。 <span class="math display">\[
\begin{aligned}
\theta_{t+1}=\theta_t-\frac{\eta}{|B|}\cdot\frac{\partial
L(f_{\theta_t}(x+\delta^*),y)}{\partial \theta},((x,y)\in B)
\end{aligned}
\]</span> 我们假装只能看到 <span class="math inline">\(B\)</span>
这个子集的数据，然后梯度下降。</p>
<h2 id="evaluating-robust-model">Evaluating robust model</h2>
<p>随机起点跑 PGD，然后看 max loss 的均值。当然，我们 <span
class="math inline">\(\Delta\)</span> 的选择不同（范数不同），会对
robust 产生影响。比如说在 <span class="math inline">\(\ell_1\)</span>
robust 的 model 不一定在 <span
class="math inline">\(\ell_2,\ell_{\infty}\)</span> 也 robust.</p>
<p>我们很难训练出一个真正的 robust model，但是我们也可以根据不完美的
adversarial training 后的模型获得额外信息。</p>
<p><img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20241228160939956.png" srcset="/img/loading.gif" lazyload alt="image-20241228160939956" style="zoom:50%;" /></p>
<hr />
<h2 id="non-robust-features">non-robust features</h2>
<p>给我们一组 <span class="math inline">\((x,y)\)</span> ，我们使用 PGD
去攻击他得到了一组 <span
class="math inline">\((x&#39;,y&#39;)\)</span>，比如说 <span
class="math inline">\(y\)</span> 是“狗”，而 <span
class="math inline">\(y&#39;\)</span> 是猫。</p>
<p>那么 <span class="math inline">\(x&#39;\)</span> 应满足： <span
class="math display">\[
\begin{aligned}
x&#39;=\mathrm{argmin}_{\| x-x&#39;\|\le \epsilon}L(x&#39;,y&#39;)
\end{aligned}
\]</span> 所以我们得到了一组看起来像“狗”，但是被分类到“猫”的图像 <span
class="math inline">\(x&#39;\)</span>。那么如果我们将 <span
class="math inline">\(x&#39;,y&#39;\)</span>
放入数据集中会得到怎样的模型呢？我们依旧会得到一个正确的模型（在原先的数据集合内正确）。</p>
这是因为模型会学到一些 "non-robust" 的特征，这是人类所看不到的。 $$
<span class="math display">\[\begin{aligned}

\end{aligned}\]</span>
<p>$$</p>
<hr />
<p>Generating non-robust feature data set.</p>
<p>假设已经有一个 robust feature 提取器 <span
class="math inline">\(g\)</span>，我们对于原先数据集中每一个图片 <span
class="math inline">\(x\)</span> ，随机生成一个 <span
class="math inline">\(g(x_r)\approx g(x)\)</span>（可以使用 GD）</p>
<p>直觉上说 <span class="math inline">\(x_r\)</span> 所拥有的 non-robust
feature 将会比较独立，而他们的 robust feature
比较类似。在新构造出的数据集上训练。</p>
<hr />
<h2 id="attack">Attack</h2>
<p>图像识别 white-box attack，用 adversarial attack 得到一个数据 <span
class="math inline">\((x,y)\)</span> 将他 compress 成一个
JPG，然后发布到网上。</p>
<p>defense models:</p>
<ul>
<li>混淆梯度
<ul>
<li>shattering gradients : undifferentiable（不可微），Numeric
instability（数值不稳定），梯度退化。<br />
</li>
<li>Stochastic Gradients : add randomness</li>
<li>Exploding and vanishing gradients :
非常神的神经网络，导致链式法则非常长，最终梯度爆炸/消失。</li>
</ul></li>
</ul>
<p>混淆梯度最终失败了。</p>
<h3 id="attack-techniques">Attack techniques</h3>
<ol type="1">
<li><p>对于不稳定，不可微的情况，是model在原先的 <span
class="math inline">\(x\)</span> 外侧套上一层壳 <span
class="math inline">\(g\)</span>，<span class="math inline">\(g\)</span>
不稳定，不可微分。且满足 <span class="math inline">\(g(x)\approx
x\)</span>。</p>
<p>我们不好计算 Loss function: <span class="math inline">\(\nabla_x
f(g(x))\)</span>，（其中 <span class="math inline">\(f\)</span> 是 loss
function）。 <span class="math display">\[
\begin{aligned}
\nabla_x f(g(x))\approx (\nabla_xf)(g(x))
\end{aligned}
\]</span></p>
<p>也就是 <span class="math inline">\(\nabla_x f(\cdot)\)</span> 在
<span class="math inline">\(g(x)\)</span> 处的取值。</p></li>
<li><p>对于随机的情况，我们可以创造多个对象，或者直接假设他的梯度是
期望方向。</p></li>
</ol>
<hr />
<h2 id="histogram">histogram</h2>
<p>我们希望找到一个有 guarantee 的 robust model。</p>
<p>假如我们现在有了要给 non-robust 的 classifier。我们假设一次询问点是
<span class="math inline">\(x\)</span>，我们构造一个关于 <span
class="math inline">\(x\)</span> 的分布（比如以 <span
class="math inline">\(x\)</span> 为圆心的球，比如以 <span
class="math inline">\(x\)</span> 为中心的 Gaussian）。</p>
<p>然后计算，对每一种颜色（in different class）: <span
class="math display">\[
w_v=\int_{v}[f(v)=i]\mathrm{Pr}(v)\mathrm{d}v
\]</span> 我们选择最大的 <span class="math inline">\(w_v\)</span>
作为答案。这个的答案记作 <span
class="math inline">\(g(v):=\mathrm{argmin}_{v} w_v\)</span>。</p>
<hr />
<p>我们希望证明 <span class="math inline">\(g\)</span> 有 robust 的
guarantee.</p>
<p>对一个点 <span
class="math inline">\(x\)</span>，我们计算，在最坏情况下使得 <span
class="math inline">\(g(x)=g(x+\delta)\)</span> 最大的 <span
class="math inline">\(|\delta|\)</span>。我们进行 greedy fill。</p>
<p>我们按照 <span
class="math inline">\(\frac{\mathrm{Pr}(v|x)}{\mathrm{Pr}(v|x+\delta)}\)</span>
从大到小染色，然后看在这种情况下， <span
class="math inline">\(g(x)\)</span> 在 <span
class="math inline">\(x+\delta\)</span> 处的积分是不是大于 <span
class="math inline">\(\frac{1}{2}\)</span>。</p>
<hr />
<h1 id="hyperparameter-optimization">hyperparameter optimization</h1>
<ol type="1">
<li>Bayesian optimization</li>
<li>Gradient descent</li>
<li>Random Search</li>
<li>Multi-armed Bandit based algorithms</li>
<li>Grid Search</li>
</ol>
<h2 id="bayesian-optimization">Bayesian Optimization</h2>
<p>我们用 Gaussian process
在函数空间中取样，最开始有一个“先验知识”（linear,
cycle...）因为最开始没有“后验知识”的，所以直接根据“先验知识”生成的概率分布
<span class="math inline">\(D\)</span>
采样。根据函数空间，我们得到了一个最优秀的函数值点 <span
class="math inline">\(x\)</span>，我们看一看 <span
class="math inline">\(x\)</span> 点的函数值 <span
class="math inline">\((x,f(x))\)</span>，然后再根据这个“后验知识”改进
Gaussian process 的 covariance 矩阵，得到新的分布 <span
class="math inline">\(D&#39;\)</span>，接着做。</p>
<h2 id="gradient-descent">Gradient Descent</h2>
<p>我们假设一个跑 <span class="math inline">\(n\)</span> 步的
GD，我们可以把它写成 <span class="math inline">\(f(x_0,\eta)\)</span>
我们需要设置超参 <span class="math inline">\(\eta\)</span>，我们希望对
<span class="math inline">\(\eta\)</span>
求导，得到一个尽量好的参数。</p>
<p>直接做需要存下来 <span class="math inline">\(w_1,\cdots,w_n\)</span>
这需要 <span class="math inline">\(n^2\)</span> 的内存，很垃圾。</p>
<p>我们考虑使用 momentum。 <span class="math display">\[
\begin{aligned}
g_{t+1}&amp;=\nabla L(w_t)&amp;(1)\\
v_{t+1}&amp;=\gamma v_t- (1-\gamma)g_{t+1}&amp;(2)\\
w_{t+1}&amp;=w_t+v_{t+1}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ &amp;(3)
\end{aligned}
\]</span> 这样，我们就可以根据 <span
class="math inline">\(w_{t+1},v_{t+1}\)</span> 反推出 <span
class="math inline">\(w_t\)</span> (3)，然后用 (1) 算出来 <span
class="math inline">\(g_{t+1}\)</span>，然后再反推出 <span
class="math inline">\(v_t\)</span> (2)。</p>
<p>总共需要： <span
class="math inline">\(T\log\left(\frac{1}{\gamma}\right)\)</span>
bits。</p>
<p>需要：变量连续。</p>
<h2 id="random-search">random search</h2>
<p>随机搜。实际有较好的效果。</p>
<h2 id="multi-armed-bandit-based-algorithms">Multi-armed Bandit based
algorithms</h2>
<p>有 <span class="math inline">\(n\)</span> 个 arm，每个变量都是
bounded 的，第 <span class="math inline">\(i\)</span>-th 老虎机的期望是
<span class="math inline">\(v_i\)</span>。</p>
<p>我们希望找到最大的那个 arm。</p>
<h3 id="sh-algorithm">SH algorithm</h3>
<p>设置一个阈值 <span class="math inline">\(B\)</span>，以及 <span
class="math inline">\(T=B/\log_2(n)\)</span>。</p>
<p>在第 <span class="math inline">\(i\)</span> 轮，假设此时剩余的集合为
<span class="math inline">\(S_i\)</span>，我们对每一个 <span
class="math inline">\(x\in S_i\)</span> 玩 <span
class="math inline">\(\frac{T}{|S_i|}\)</span> 次，我们选取均值最大的
<span class="math inline">\(\frac{|S_i|}{2}\)</span> 个。</p>
<p>这样，每次减半，最终只会玩 <span
class="math inline">\(\log_2(n)\)</span> 轮，总共 <span
class="math inline">\(B\)</span> 次。</p>
<hr />
<p>我们给出证明：WLOG，我们假设 <span
class="math inline">\(v_1&gt;v_2\ge v_3\cdots\ge v_n\)</span>。假设
<span class="math inline">\(\Delta_i=v_1-v_i\)</span>。</p>
<h3 id="theorem">theorem</h3>
<p>选 <span class="math inline">\(B=O(H_2\log n\log{\frac{\log
m}{\delta}})\)</span> ，w.p. <span class="math inline">\(\ge
1-\delta\)</span> 我们可以得到最优老虎机。</p>
<p><span
class="math inline">\(H_2:=\max_{i=2}^n\frac{i}{\Delta_i^2}\)</span>.</p>
<ol type="1">
<li><p>我们首先给出： <span class="math inline">\(\mathrm{Pr}(\hat
v_1&lt;\hat v_i)\)</span> 的 upper bound，我们假设老虎机给的是 <span
class="math inline">\([0,1]\)</span> 随机分布。</p>
<p>根据 Hoeffding Inequality: 一个均值为 <span
class="math inline">\(\mu\)</span>，且有上下界 <span
class="math inline">\([a,b]\)</span> 的随机变量，sample <span
class="math inline">\(n\)</span> 次后的均值记为：<span
class="math inline">\(\overline{x}\)</span>，则： <span
class="math display">\[
\begin{aligned}
\Pr[\overline{x}-\E[\overline x]\ge t]\le
\exp\left(-\frac{-2t^2n^2}{n(b-a)^2}\right)=\exp\left(-\frac{2t^2n}{(b-a)^2}\right)
\end{aligned}
\]</span> 我们可以扩展到 <span class="math inline">\(a_i\le x_i\le
b_i\)</span>，<span class="math inline">\(\overline x=\frac{1}{n}\sum
x_i\)</span>。</p>
<p>那么： <span class="math display">\[
\begin{aligned}
\mathrm{Pr}[\hat v_1\le \hat v_i]=\mathrm{Pr}[\hat v_1-\hat v_i\le 0]\le
\exp\left(-\frac{2\Delta_i^2T}{(1-(-1))^2}\right)=\exp\left(-\frac{\Delta^2_iT}{2}\right)
\end{aligned}
\]</span> 其中：<span class="math inline">\(T=\frac{B}{\log
n|S_i|}\)</span>。</p></li>
<li><p>计算出被干掉的期望，然后使用 Markov 不等式。（我们将前 <span
class="math inline">\(\frac{1}{4}\)</span>
hide，认为他们可以超过第一个人。bound 剩下的 <span
class="math inline">\(\frac{3}{4}\)</span>） <span
class="math display">\[
\begin{aligned}
\E[\#\hat v_i\ge \hat v_1]=\sum_{j\in
S_i&#39;}\exp\left(-\frac{\Delta_i^2B}{2|S_i|\log n}\right)\le
|S_i&#39;|\exp\left(-\frac{\Delta_{|S_i|}^2B}{2|S_i|\log n}\right)\\
\mathrm{Pr}\left[(\#\hat v_i\ge \hat v_1)\ge \frac{1}{4}|S_i|\right]\le
3\exp\left(-\frac{\Delta_{|S_i|}^2B}{2|S_i|\log n}\right)
\end{aligned}
\]</span> union bound一下： <span class="math display">\[
\begin{aligned}
\mathrm{Pr[lost]}\le 3\log
n\exp\left(-\frac{\Delta_{|S_i|}^2B}{2|S_i|\log n}\right)
\end{aligned}
\]</span> 故，我们 <span class="math inline">\(B=O\left(2H_2\log
n\log\left(\frac{\log n}{\delta}\right)\right)\)</span> 可以使得：<span
class="math inline">\(\mathrm{Pr[lost]}\le \delta\)</span>。</p></li>
</ol>
<hr />
<h3 id="sh-in-hyperparameter-learning">SH in hyperparameter
learning</h3>
<p>我们考虑一个GD（或者别的什么model训练的）过程，假设我们随机选择
hyperparameter 得到了 <span class="math inline">\(S_1\)</span>
个模型。</p>
<p>我们假设他们都会收敛，我们还是希望找到最好的那个 model。</p>
<p>我们假设： <span class="math display">\[
\begin{aligned}
\forall y\ge x,|f_{i}(k)-v_i|\le \gamma_i(t)
\end{aligned}
\]</span> 并且 <span class="math inline">\(\gamma_i(t)\)</span>
单调减。</p>
<p>也就是：</p>
<p><img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20250102005517290.png" srcset="/img/loading.gif" lazyload alt="image-20250102005517290" style="zoom:50%;" /></p>
<p>黑线表示 <span class="math inline">\(f_i(k)\)</span> 即，模型 <span
class="math inline">\(i\)</span> 训练 <span
class="math inline">\(k\)</span> 步后的 loss，而橙色是 <span
class="math inline">\(\gamma_i(t)+v_i\)</span>。</p>
<p>我们考虑 <span class="math inline">\(\gamma_i(t)\)</span> 的反函数
<span class="math inline">\(\gamma_i^{-1}(\alpha)\)</span>
可以理解成，何时“陷入了” <span class="math inline">\(\alpha\)</span>
邻域。不妨假设 <span
class="math inline">\(\gamma(t)=\max_{i=1}^n\gamma_i(t)\)</span>。</p>
<p>我们考虑这样一件事情，如果此时 <span
class="math inline">\(k\)</span>（训练轮数）大于 <span
class="math inline">\(\gamma_i^{-1}(\frac{v_i-v_1}{2}),\gamma_1^{-1}(\frac{v_i-v_1}{2})\)</span>，那么我们就可以认定
<span class="math inline">\(f_1\)</span> 和 <span
class="math inline">\(f_i\)</span> 被分开了。</p>
<hr />
<h3 id="theorem-1">theorem</h3>
<p>如果我们的 <span class="math inline">\(B\)</span> 满足： <span
class="math display">\[
\begin{aligned}
B\ge
2\log_2n\left(n+\sum_{i=2}^n\gamma^{-1}\left(\frac{v_i-v_1}{2}\right)\right)
\end{aligned}
\]</span> 可以发现 <span class="math inline">\(\gamma^{-1}\)</span>
也具有单调性。</p>
<p>我们可以发现： <span class="math display">\[
\begin{aligned}
T=2\left(n+\sum_{i=2}^n\gamma^{-1}\left(\frac{v_i-v_1}{2}\right)\right)\\
\frac{T}{|S_i|}\ge \gamma^{-1}\left(\frac{v_{|S_i|/2+1}-v_1}{2}\right)
\end{aligned}
\]</span> 所以可以分开。可以发现，这个 <span
class="math inline">\(\gamma\)</span> 性质巨强无比，所以没有概率。</p>
<hr />
<h2 id="proxylessnas">ProxyLessNAS</h2>
<p>我们现在有一个很深很深的神经网络的层，我们需要去决定每一层都是什么（<span
class="math inline">\(3\times 3\)</span>,<span
class="math inline">\(5\times5\)</span>,identity... and so
on）每一层内部的权值。以及我们怎么选取这一层。</p>
<hr />
<p>假设第 <span class="math inline">\(i\)</span> 个 component 返回 <span
class="math inline">\(o_i(x)\)</span>。</p>
<p>那么 One-shot 做法是：<span
class="math inline">\(f(x)=\sum_{i=1}^no_i(x)\)</span> 愚蠢的很。</p>
<p>DARTS 做法是： <span
class="math inline">\(f(x)=\sum_{i=1}^n\frac{\exp(\alpha_i)}{\sum
\exp(\alpha_i)}o_i(x)\)</span>。</p>
<p>这个做法的内存效力很差，我们改进做法是，每次以 <span
class="math inline">\(p_i=\frac{\exp(\alpha_i)}{\sum
\exp(\alpha_i)}\)</span> 的概率随机选一个 component <span
class="math inline">\(i\)</span>，然后返回 <span
class="math inline">\(o_i(x)\)</span>，并且此时，不将 <span
class="math inline">\(o_j,j\not=i\)</span> 放入内存。（省内存）</p>
<hr />
<p>形式化的，我们定义一个随机变量 <span
class="math inline">\(g=[1,0,\cdots,0] w.p.p_1;=[0,1,\cdots,0],w.p.
p_2\)</span> 以此类推。</p>
<p><img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20250102015028509.png" srcset="/img/loading.gif" lazyload alt="image-20250102015028509" style="zoom:50%;" /></p>
<p>然后我们的返回函数是： <span class="math display">\[
\begin{aligned}
f(x)=\sum_{i=1}^Ng_io_i(x)
\end{aligned}
\]</span> 其中 <span class="math inline">\(g_i\)</span> sample 出来的
<span class="math inline">\(g\)</span> 的第 <span
class="math inline">\(i\)</span> 位的值。和上述方法一致。</p>
<hr />
<h3 id="如何训练">如何训练？</h3>
<p>我们训练神经网络内部时，froze <span
class="math inline">\(\alpha_i\)</span>，同时训练 <span
class="math inline">\(\alpha_i\)</span> 时，froze 神经网络内部值。</p>
<p>如何训练 <span
class="math inline">\(\alpha_i\)</span>，他并不显式出现在 loss function
里？ <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial \alpha_i}=\sum_{j=1}^N\frac{\partial
L}{\partial p_j}\frac{\partial p_j}{\partial
\alpha_i}\approx\sum_{j=1}^N\frac{\partial L}{\partial
g_j}\frac{\partial p_j}{\partial\alpha_i}=\sum_{j=1}^N\frac{\partial
L}{\partial g_j}p_j(\delta_{i,j}-p_i)
\end{aligned}
\]</span> 因为： <span class="math display">\[
\begin{aligned}
\frac{\partial\frac{\exp(a_j)}{\sum\exp(\alpha_k)}}{\partial\alpha_i}&amp;=-\frac{\exp(a_j)}{(\sum\exp(\alpha_k))^2}\times
\exp(a_i)\\
&amp;=-p_jp_i\\
\frac{\partial\frac{\exp(a_i)}{\sum\exp(\alpha_k)}}{\partial\alpha_i}&amp;=-\frac{\exp(a_i)}{(\sum\exp(\alpha_k))^2}\times
\exp(a_i)+\frac{\exp(a_i)}{\sum\exp(\alpha_k)}\\
&amp;=p_i(1-p_i)
\end{aligned}
\]</span> Loss function 里有 <span class="math inline">\(g_i\)</span>
所以直接假装 <span class="math inline">\(g_1,\cdots g_n\)</span>
全是独立变量求导即可。</p>


  
</article>

              </div>
            </div>
          </div>
        </div>
      </div>
    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
